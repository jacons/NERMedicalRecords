{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ViT-G1mIgSRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0fzLyOZKRH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "from pandas import DataFrame\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD,AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, time, os\n",
        "import seaborn as sns\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# folder = \"/content/drive/Othercomputers/Il mio Laptop/Universita/[IA] Artificial Intelligence/[HLT] Human Language Technologies/NERforMedicalRecords/\"\n",
        "folder = \"/content/drive/MyDrive/NERforMedicalRecords/\"\n",
        "bert  = \"dbmdz/bert-base-italian-xxl-cased\"\n",
        "\n",
        "# list of file to take into account\n",
        "#folder+\"Corpus/esami.a.iob\", folder+\"Corpus/esami.b.iob\"\n",
        "datasets = [folder+\"Corpus/anamnesi.a.iob\", folder+\"Corpus/anamnesi.b.iob\"]\n",
        "\n",
        "# label to entity tag to recognize\n",
        "labels_to_ids = {'B-ACTI': 0, 'B-BODY': 1, 'B-DISO': 2, 'B-DRUG': 3, 'B-SIGN': 4, 'B-TREA': 5, 'I-ACTI': 6,\n",
        "                 'I-BODY': 7, 'I-DISO': 8, 'I-DRUG': 9, 'I-SIGN': 10, 'I-TREA': 11, 'O': 12}\n",
        "\n",
        "ids_to_labels = {0: 'B-ACTI', 1: 'B-BODY', 2: 'B-DISO', 3: 'B-DRUG', 4: 'B-SIGN', 5: 'B-TREA', 6: 'I-ACTI',\n",
        "                 7:'I-BODY', 8:'I-DISO', 9: 'I-DRUG',10: 'I-SIGN', 11: 'I-TREA', 12: 'O'}\n",
        "\n",
        "total_labels = 13\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert)\n",
        "\n",
        "def align_label(token: list, labels: str):\n",
        "    \n",
        "  labels = labels.split() # Trasforming a string of label into array\n",
        "\n",
        "  label_ids = [] # Aligned labels\n",
        "\n",
        "  # We can all ids in the token and we try to associate to a label  \n",
        "  for word_idx in token:\n",
        "  \n",
        "    # typically when we encounter [CLS]\n",
        "    if word_idx is None: label_ids.append(-100) \n",
        "    else:\n",
        "      try: # We try to associate a label\n",
        "        label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "      except:\n",
        "        label_ids.append(-100)\n",
        "\n",
        "  return label_ids"
      ],
      "metadata": {
        "id": "ADDdl3OklbHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Parser:\n",
        "  def __init__(self, paths: list):\n",
        "    ## ========== PARAMETERS ==========\n",
        "    self.tr_size:float = 0.8\n",
        "    self.vl_size:float = 0.1\n",
        "    self.ts_size:float = 0.1\n",
        "    ## ========== PARAMETERS ==========\n",
        "    self.dataset = [] # List of phrases and labels\n",
        "\n",
        "    # For each file in a list, we parse a file formatted in ConLL\n",
        "    for file_ in paths: self.read_conll(file_)\n",
        "\n",
        "  def read_conll(self, path:str) -> None:\n",
        "\n",
        "    nphrases = 0\n",
        "    print(\"File: \",os.path.basename(path))\n",
        "\n",
        "    with open(path) as f: \n",
        "      phrase,labels = [], [] # lists that contain the words and labels\n",
        "\n",
        "      for line in f.readlines(): # reads word by word\n",
        "        line = line.split() # Trasform a line into array\n",
        "\n",
        "        if len(line) == 0:\n",
        "          # if the \"phrase\" contains at least one word we add to dataset\n",
        "          if (len(phrase)!= 0) & (len(labels)!= 0):\n",
        "            self.dataset.append((\" \".join(phrase),\" \".join(labels)))\n",
        "            nphrases += 1\n",
        "          phrase,labels = [], []\n",
        "\n",
        "        elif line[0] != \"-DOCSTART-\":\n",
        "          phrase.append(line[0]) # Not lemmatized word\n",
        "          labels.append(line[3]) # label that corresponding to the word\n",
        "      print(\"\\tNumber of phrases made: \",nphrases)\n",
        "\n",
        "  def holdout(self, size: float = 0.5) -> DataFrame:  \n",
        "\n",
        "    \"\"\"\n",
        "    In this phase we transfom a list of pairs (phrase,label) into \"holdout\" dataframe used to model selection\n",
        "      1) Create a unique dataframe.\n",
        "      2) Remove the duplicate (useless for our scope, improve the data dimention)\n",
        "      3) Sampling the dataset in order to work with a subset of all data avaiable\n",
        "      4) Dividing the final dataset base on holdout technique.\n",
        "    \"\"\"\n",
        "    dt = pd.DataFrame(self.dataset,columns=[\"tokens\",\"labels\"]).drop_duplicates().sample(frac=size, random_state=42)\n",
        "    length = len(dt)\n",
        "\n",
        "    tr = int(self.tr_size * length)\n",
        "    print(\"\\nTotal number of phrases: \", length, \" (tr): \", tr, \" (vl): \", int(self.vl_size * length), \" (ts): \", int(self.ts_size * length))\n",
        "\n",
        "    return np.split(dt,[tr, int((self.tr_size + self.vl_size) * length)])"
      ],
      "metadata": {
        "id": "5LCyIt1yXqDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NerDataset(Dataset):\n",
        "  # We try preprocess the data as much as possible.\n",
        "  def __init__(self, dataset: DataFrame):\n",
        "\n",
        "    self.input_ids, self.mask, self.labels = [], [], []\n",
        "\n",
        "    for _,row in dataset.iterrows():\n",
        "      # Apply the tokenization at each row\n",
        "      token_text = tokenizer(row[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "      label_ids = align_label(token_text.word_ids(),row[1])\n",
        "\n",
        "      # moving the result on GPU\n",
        "      self.input_ids.append(token_text['input_ids'].squeeze(0).to(\"cuda:0\"))\n",
        "      self.mask.append(token_text['attention_mask'].squeeze(0).to(\"cuda:0\"))\n",
        "      self.labels.append(torch.LongTensor(label_ids).to(\"cuda:0\"))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.mask[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "Og5GMB1zi-Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "    def __init__(self,frozen=True):\n",
        "        super(BertModel, self).__init__()\n",
        "        self.bert = AutoModelForMaskedLM.from_pretrained(bert, num_labels = total_labels)\n",
        "        if frozen:\n",
        "          for param in self.bert.bert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2XSDP0I6joeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, df_train: DataFrame, df_val: DataFrame, batch_size: int, lr: float,\n",
        "          max_epochs: int, earlyS: bool = True, cache: bool = True,name:str = \"model.py\"):\n",
        "  \n",
        "  # We create a iterator for training e validation dataset\n",
        "  tr = DataLoader(NerDataset(df_train), batch_size=batch_size)  \n",
        "  vl = DataLoader(NerDataset(df_val), batch_size=batch_size)\n",
        "  tr_size, vl_size = len(tr), len(vl)\n",
        "\n",
        "  earlyS_flag:int = 0\n",
        "  epoch:int = 0\n",
        "  previuous_vl:float = float(\"inf\")\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=lr,weight_decay=1e-2)\n",
        "  \n",
        "  while (epoch < max_epochs) & (earlyS_flag <= 1): \n",
        "\n",
        "    loss_train, loss_val = 0, 0\n",
        "    \n",
        "    # ========== Training Phase ==========\n",
        "    model.train()\n",
        "    for input_id, mask, tr_label in tqdm(tr):\n",
        "      optimizer.zero_grad()\n",
        "      loss, _ = model(input_id, mask, tr_label)\n",
        "      loss_train += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()      \n",
        "    # ========== Training Phase ==========\n",
        "\n",
        "    if cache:\n",
        "      torch.save(model.state_dict(), \"./\"+name)\n",
        "\n",
        "    # ========== Validation Phase ==========\n",
        "    model.eval() # Validation phase\n",
        "    for input_id, mask, val_label in vl:\n",
        "        loss, _ = model(input_id, mask, val_label)\n",
        "        loss_val += loss.item()    \n",
        "    # ========== Validation Phase ==========\n",
        "\n",
        "    tr_loss, val_loss = (loss_train / tr_size), (loss_val / vl_size)\n",
        "\n",
        "    ## Early stopping\n",
        "    if earlyS:\n",
        "      if (previuous_vl < val_loss): earlyS_flag += 1\n",
        "      previuous_vl = val_loss\n",
        "\n",
        "    print(f'Epochs: {epoch + 1} | Loss: {tr_loss: .3f} | Val_Loss: {val_loss: .3f}')\n",
        "    epoch += 1"
      ],
      "metadata": {
        "id": "4tsxc14Uq52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = Parser(datasets).holdout(size=1)"
      ],
      "metadata": {
        "id": "k_1hkX5vX6i_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "951aa522-c296-46b6-d22d-e7245d57de5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File:  anamnesi.a.iob\n",
            "\tNumber of phrases made:  72297\n",
            "File:  anamnesi.b.iob\n",
            "\tNumber of phrases made:  72297\n",
            "\n",
            "Total number of phrases:  48433  (tr):  38746  (vl):  4843  (ts):  4843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel().to(\"cuda:0\")\n",
        "train(model, df_train, df_val, batch_size=5, lr=5e-3, max_epochs=5, cache=True)"
      ],
      "metadata": {
        "id": "hXZ-COyxtgjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# il modello caricato sul drive ha 2 epoche "
      ],
      "metadata": {
        "id": "z36_kf5Q0Cs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! cp -r \"/content/drive/Othercomputers/Il mio Laptop/Universita/[IA] Artificial Intelligence/[HLT] Human Language Technologies/NERforMedicalRecords/tmp/model.pt\" \"/content/model.pt\" \n",
        "! cp -r \"/content/drive/MyDrive/NERforMedicalRecords/tmp/model.pt\" \"/content/model.pt\" "
      ],
      "metadata": {
        "id": "gXqZLsKoP9oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel().to(\"cuda:0\")\n",
        "model.load_state_dict(torch.load(\"model.pt\"))"
      ],
      "metadata": {
        "id": "b_yFCMvzk-Ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f2f8cf-f4cc-4997-e560-82bb1569fd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODELLO ALLENATO PER "
      ],
      "metadata": {
        "id": "ltBDvkjEkxcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, df_train, df_val, batch_size=5, lr=5e-3, max_epochs=5, cache=True, name=\"modelV2.pt\")"
      ],
      "metadata": {
        "id": "sH4KkKmEHWYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# par1 = {\"batch_size\": 5, \"lr\": 5e-3, \"max_epoch\": 5, \"weigth_decay\": 1e-2}\n",
        "# metric = pd.DataFrame([accuracy.numpy(),precision.numpy(),recall.numpy(),f1.numpy()],columns=['B-ACTI','B-BODY','B-DISO','B-DRUG','B-SIGN','B-TREA','I-ACTI','I-BODY','I-DISO','I-DRUG','I-SIGN','I-TREA','O'],\\\n",
        "#              index=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])"
      ],
      "metadata": {
        "id": "e75BHJC0E2I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Email di questa sera:\n",
        "2) Parlare del parser costruito\n",
        "3) Parlare del model selection (come intendo fare la grid e k-fold)\n",
        "4) Parlare del fatto di mantere untite tutte le entity\n",
        "5) Che sto utilizzando solo anamnesi\n",
        "6) Dei risultati ottenuti\n",
        "7) Di come effettuare la valutazione \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Da fare:\n",
        "\n",
        "-) Implementare sistema per la valutazione di singole frasi\n",
        "-) Implementare K-Fold and Grid search\n",
        "-) Ulteriori implementazioni isolando solo dei gruppi di entity invece che usarle tutte insieme\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Problemi:\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I2nqdrOGZtHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r \"/content/modelV2.pt\" \"/content/drive/MyDrive/NERforMedicalRecords/tmp/modelV2.pt\""
      ],
      "metadata": {
        "id": "BEUP4mQGlb_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}