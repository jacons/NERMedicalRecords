{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ViT-G1mIgSRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0fzLyOZKRH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "from pandas import DataFrame\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD,AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, time, os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder = \"/content/drive/MyDrive/NERforMedicalRecords/\"\n",
        "bert  = \"dbmdz/bert-base-italian-xxl-cased\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of file to take into account\n",
        "\n",
        "#folder+\"Corpus/esami.a.iob\", folder+\"Corpus/esami.b.iob\"\n",
        "datasets = [folder+\"Corpus/anamnesi.a.iob\", folder+\"Corpus/anamnesi.b.iob\"]\n",
        "\n",
        "# label to entity tag to recognize\n",
        "labels_to_ids = {'B-ACTI': 0, 'B-BODY': 1, 'B-DISO': 2, 'B-DRUG': 3, 'B-SIGN': 4, 'B-TREA': 5, 'I-ACTI': 6,\n",
        "                 'I-BODY': 7, 'I-DISO': 8, 'I-DRUG': 9, 'I-SIGN': 10, 'I-TREA': 11, 'O': 12}\n",
        "\n",
        "ids_to_labels = {0: 'B-ACTI', 1: 'B-BODY', 2: 'B-DISO', 3: 'B-DRUG', 4: 'B-SIGN', 5: 'B-TREA', 6: 'I-ACTI',\n",
        "                 7:'I-BODY', 8:'I-DISO', 9: 'I-DRUG',10: 'I-SIGN', 11: 'I-TREA', 12: 'O'}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert)\n",
        "\n",
        "def align_label(token: list, labels: str):\n",
        "    \n",
        "  labels = labels.split() # Trasforming a string of label into array\n",
        "\n",
        "  # We take into cosideration the previous word to identify if the id is already seen\n",
        "  previous_id = None\n",
        "  label_ids = [] # Aligned labels\n",
        "\n",
        "  # We can all ids in the token and we try to associate to a label  \n",
        "  for word_idx in token:\n",
        "  \n",
        "    # typically when we encounter [CLS]\n",
        "    if word_idx is None: label_ids.append(-100) \n",
        "    else:\n",
        "      try: # We try to associate a label\n",
        "        label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "      except:\n",
        "        label_ids.append(-100)\n",
        "    previous_id = word_idx \n",
        "\n",
        "  return label_ids"
      ],
      "metadata": {
        "id": "ADDdl3OklbHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConLL_parser:\n",
        "  def __init__(self, paths: list):\n",
        "    ## ========== PARAMETERS ==========\n",
        "    self.tr_size:float = 0.8\n",
        "    self.vl_size:float = 0.1\n",
        "    self.ts_size:float = 0.1\n",
        "    ## ========== PARAMETERS ==========\n",
        "    self.dataset = [] # List of phrases and labels\n",
        "\n",
        "    # For each file in a list, we parse a file formatted in ConLL\n",
        "    for file_ in paths: self.read_conll(file_)\n",
        "\n",
        "  def read_conll(self, path:str) -> None:\n",
        "\n",
        "    nphrases = 0\n",
        "    print(\"File: \",os.path.basename(path))\n",
        "\n",
        "    with open(path) as f: \n",
        "      phrase,labels = [], [] # lists that contain the words and labels\n",
        "\n",
        "      for line in f.readlines(): # reads word by word\n",
        "        line = line.split() # Trasform a line into array\n",
        "\n",
        "        if len(line) == 0:\n",
        "          # if the \"phrase\" contains at least one word we add to dataset\n",
        "          if (len(phrase)!= 0) & (len(labels)!= 0):\n",
        "            self.dataset.append((\" \".join(phrase),\" \".join(labels)))\n",
        "            nphrases += 1\n",
        "          phrase,labels = [], []\n",
        "\n",
        "        elif line[0] != \"-DOCSTART-\":\n",
        "          phrase.append(line[0]) # Not lemmatized word\n",
        "          labels.append(line[3]) # label that corresponding to the word\n",
        "      print(\"\\tNumber of phrases made: \",nphrases)\n",
        "\n",
        "  def holdout(self, size: float = 0.5) -> DataFrame:  \n",
        "\n",
        "    \"\"\"\n",
        "    In this phase we transfom a list of pairs (phrase,label) into \"holdout\" dataframe used to model selection\n",
        "      1) Create a unique dataframe.\n",
        "      2) Remove the duplicate (useless for our scope, improve the data dimention)\n",
        "      3) Sampling the dataset in order to work with a subset of all data avaiable\n",
        "      4) Dividing the final dataset base on holdout technique.\n",
        "    \"\"\"\n",
        "    dt = pd.DataFrame(self.dataset,columns=[\"tokens\",\"labels\"]).drop_duplicates().sample(frac=size, random_state=42)\n",
        "    length = len(dt)\n",
        "\n",
        "    tr = int(self.tr_size * length)\n",
        "    print(\"\\nTotal number of phrases: \", length, \" (tr): \", tr, \" (vl): \", int(self.vl_size * length), \" (ts): \", int(self.ts_size * length))\n",
        "\n",
        "    return np.split(dt,[tr, int((self.tr_size + self.vl_size) * length)])"
      ],
      "metadata": {
        "id": "5LCyIt1yXqDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  # We try preprocess the data as much as possible.\n",
        "  def __init__(self, dataset: DataFrame):\n",
        "\n",
        "    self.input_ids, self.mask, self.labels = [], [], []\n",
        "\n",
        "    for _,row in dataset.iterrows():\n",
        "      # Apply the tokenization at each row\n",
        "      token_text = tokenizer(row[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "      label_ids = align_label(token_text.word_ids(),row[1])\n",
        "\n",
        "      # moving the result on GPU\n",
        "      self.input_ids.append(token_text['input_ids'].squeeze(0).to(\"cuda:0\"))\n",
        "      self.mask.append(token_text['attention_mask'].squeeze(0).to(\"cuda:0\"))\n",
        "      self.labels.append(torch.LongTensor(label_ids).to(\"cuda:0\"))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.mask[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "Og5GMB1zi-Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "    def __init__(self,frozen=True):\n",
        "        super(BertModel, self).__init__()\n",
        "        self.bert = AutoModelForMaskedLM.from_pretrained(bert,num_labels=13)\n",
        "        if frozen:\n",
        "          for param in self.bert.bert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2XSDP0I6joeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSelection():\n",
        "  def __init__(self,grid_list:list):\n",
        "    pass"
      ],
      "metadata": {
        "id": "9KgxIyPN169i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "par1 = {\"batch_size\": 5, \"lr\": 5e-3, \"max_epoch\": 5, \"weigth_decay\": 1e-2}"
      ],
      "metadata": {
        "id": "dGxO_MsS4d9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, df_train: DataFrame, df_val: DataFrame, batch_size: int, lr: float,\n",
        "          max_epochs: int, earlyS: bool = True, cache: bool = True):\n",
        "  \n",
        "  # We create a iterator for training e validation dataset\n",
        "  tr = DataLoader(MyDataset(df_train), batch_size=batch_size)  \n",
        "  vl = DataLoader(MyDataset(df_val), batch_size=batch_size)\n",
        "  tr_size, vl_size = len(tr), len(vl)\n",
        "\n",
        "  earlyS_flag:int = 0\n",
        "  epoch:int = 0\n",
        "  previuous_vl:float = float(\"inf\")\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=lr,weight_decay=1e-2)\n",
        "  \n",
        "  while (epoch < max_epochs) & (earlyS_flag <= 1): \n",
        "\n",
        "    loss_train, loss_val = 0, 0\n",
        "    \n",
        "    # ========== Training Phase ==========\n",
        "    model.train()\n",
        "    for input_id, mask, tr_label in tqdm(tr):\n",
        "      optimizer.zero_grad()\n",
        "      loss, _ = model(input_id, mask, tr_label)\n",
        "      loss_train += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()      \n",
        "    # ========== Training Phase ==========\n",
        "\n",
        "    if cache:\n",
        "      torch.save(model.state_dict(), \"./model.pt\")\n",
        "\n",
        "    # ========== Validation Phase ==========\n",
        "    model.eval() # Validation phase\n",
        "    for input_id, mask, val_label in vl:\n",
        "        loss, _ = model(input_id, mask, val_label)\n",
        "        loss_val += loss.item()    \n",
        "    # ========== Validation Phase ==========\n",
        "\n",
        "    tr_loss, val_loss = (loss_train / tr_size), (loss_val / vl_size)\n",
        "\n",
        "    ## Early stopping\n",
        "    if earlyS:\n",
        "      if (previuous_vl < val_loss): earlyS_flag += 1\n",
        "      previuous_vl = val_loss\n",
        "\n",
        "    print(f'Epochs: {epoch + 1} | Loss: {tr_loss: .3f} | Val_Loss: {val_loss: .3f}')\n",
        "    epoch += 1"
      ],
      "metadata": {
        "id": "4tsxc14Uq52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val, df_test = ConLL_parser(datasets).holdout(size=1)"
      ],
      "metadata": {
        "id": "k_1hkX5vX6i_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1695c9-d65d-4d6c-8291-8f55f76fd87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File:  anamnesi.a.iob\n",
            "\tNumber of phrases made:  72297\n",
            "File:  anamnesi.b.iob\n",
            "\tNumber of phrases made:  72297\n",
            "\n",
            "Total number of phrases:  48433  (tr):  38746  (vl):  4843  (ts):  4843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel().to(\"cuda:0\")\n",
        "train(model, df_train, df_val, batch_size=5, lr=5e-3, max_epochs=5, cache=True)"
      ],
      "metadata": {
        "id": "hXZ-COyxtgjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# il modello caricato sul drive ha 2 epoche "
      ],
      "metadata": {
        "id": "z36_kf5Q0Cs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Email di questa sera:\n",
        "1) Parlare dei caratteri accentati\n",
        "2) Parlare del parser costruito\n",
        "3) Parlare del model selection (come intendo fare la grid e k-fold)\n",
        "4) Parlare del fatto di mantere untite tutte le entity\n",
        "5) Che sto utilizzando solo anamnesi\n",
        "6) Dei risultati ottenuti\n",
        "7) Di come effettuare la valutazione \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "  Dato un modello allenato valutare le sue prestazioni con \n",
        "  1) Matrice di confusione\n",
        "  2) Precision,Recall,Accuraracy,F1\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Da fare:\n",
        "\n",
        "-) Gestire i caratteri accentati \n",
        "-) Implementare il sistema per continuare il traing caricando il modello da file\n",
        "-) Implementare sistema per la valutazione di singole frasi\n",
        "-) Implementare K-Fold and Grid search\n",
        "-) Ulteriori implementazioni isolando solo dei gruppi di entity invece che usarle tutte insieme\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Problemi:\n",
        "1) Carattere ¿ nelle file di esami\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I2nqdrOGZtHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r \"/content/model.pt\" \"/content/drive/MyDrive/NERforMedicalRecords/tmp/model.pt\""
      ],
      "metadata": {
        "id": "gXqZLsKoP9oT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}