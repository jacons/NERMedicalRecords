{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0fzLyOZKRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ad1b03-6aa5-4964-bacc-23a96839823c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder = \"drive/Othercomputers/Il mio Laptop/Universita/[IA] Artificial Intelligence/[HLT] Human Language Technologies/project/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = []\n",
        "\n",
        "# We chose to avoid the lemmatized version of word, because the berts model work better with the raw version of word\n",
        "datasets.append(pd.read_csv(folder+\"Corpus/anamnesi.a.iob\",sep=\"\\t\",\n",
        "                  names=[\"word\",\"entity\"],usecols=[0,3]).astype({'word': 'string','entity': 'string'}))\n",
        "\n",
        "datasets.append(pd.read_csv(folder+\"Corpus/anamnesi.b.iob\",sep=\"\\t\",\n",
        "                  names=[\"word\",\"entity\"],usecols=[0,3]).astype({'word': 'string','entity': 'string'}))\n",
        "\n",
        "datasets.append(pd.read_csv(folder+\"Corpus/esami.a.iob\",sep=\"\\t\",\n",
        "                  names=[\"word\",\"entity\"],usecols=[0,3]).astype({'word': 'string','entity': 'string'}))\n",
        "\n",
        "datasets.append(pd.read_csv(folder+\"Corpus/esami.b.iob\",sep=\"\\t\",\n",
        "                  names=[\"word\",\"entity\"],usecols=[0,3]).astype({'word': 'string','entity': 'string'}))\n"
      ],
      "metadata": {
        "id": "1VPPDDNPvBiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildDataset(datasets : list, file_name : str) -> None:\n",
        "\n",
        "  # ===== Parameters =====\n",
        "  separator = \"%:%\" # Separate the text and label in the temporary dataset\n",
        "  tmp_file = \"tmp.csv\" # name of temporary file\n",
        "  stop_char = [\".\",\";\",\":\"] # list of character used to create a phrase\n",
        "  char_per_phrase = 300 # Max number of words per phrase\n",
        "  # ===== Parameters =====\n",
        "\n",
        "  try:\n",
        "    os.remove(file_name) # Remove old dataset\n",
        "  except:\n",
        "    None\n",
        "  \n",
        "  f = open(tmp_file, \"a\") # Store a temporary result on file\n",
        "  nds = 0 # Count of number of dataset\n",
        "\n",
        "  for df in datasets:\n",
        "\n",
        "    print(\"Dataset: \",nds)\n",
        "    string,labels = \"\",\"\"\n",
        "    nphrases = 0 # Number of phrased captured\n",
        "    nwords = 0 # Number of word in a phrase\n",
        "    flag = False # True if it's found a significant entity tag\n",
        "\n",
        "    # ===== Remove the meaningless values =====\n",
        "    first = len(df) # Number of rows before the elimination\n",
        "    df.dropna(how='any',axis=0,inplace=True)\n",
        "    df.drop(df[df.word == \"-DOCSTART-\"].index,inplace=True)\n",
        "    print(\"\\tRow deleted from raw-dataset: \",first-len(df))\n",
        "    print(\"\\tNum of word: \" ,len(df))\n",
        "    # ===== Remove the meaningless values =====\n",
        "\n",
        "    # ===== Build and Concat the phrases =====\n",
        "    # Iterate all row in the dataframe (each row corresponding to a word and label)\n",
        "    for _,row in df.iterrows():\n",
        "      # We concatenate each word with the actual string, the same things with label\n",
        "      string += \" \" + row[0]\n",
        "      labels  += \" \" + row[1]\n",
        "      nwords += 1\n",
        "\n",
        "      if row[1] != \"O\": # True if it's found a significant entity tag\n",
        "        flag = True\n",
        "\n",
        "      # If we encouter these chars or the number of word exceed 512\n",
        "      # means that the phrase is finished so we write the \n",
        "      # phrase on the file and we clear the string and label buffers\n",
        "      if (row[0] in stop_char) | (nwords >= char_per_phrase):\n",
        "\n",
        "        # If the flag is true means that inside the phrase there is at least\n",
        "        # a signigificant entity, so we take it into consideration, otherwise,\n",
        "        # we do not store it. (Reduce the dimentions of database)\n",
        "        if flag : \n",
        "          f.write(string + separator + labels + \"\\n\")\n",
        "          nphrases += 1\n",
        "        string,labels = \"\",\"\"\n",
        "        nwords = 0\n",
        "        flag = False\n",
        "\n",
        "    print(\"\\tNumber of phrases :\",nphrases)\n",
        "    print(\"\")\n",
        "    nds += 1\n",
        "    # ===== Build and Concat the phrases =====\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  # ===== Remove the duplicate  =====\n",
        "  # After we have finished to write the file we load it \n",
        "  # in order to remove the duplicate rows \n",
        "  dataset = pd.read_csv(tmp_file,sep=separator,\n",
        "                        names=[\"text\",\"labels\"],engine=\"python\")\n",
        "  \n",
        "  os.remove(tmp_file) # Removing the temporary file\n",
        "\n",
        "  first = len(dataset) # Number of rows before the elimination\n",
        "  print(\"Number of phrased achieved: \",first)\n",
        "  dataset.drop_duplicates(inplace=True)\n",
        "  print(\"Number of duplicated deleted: \",first - len(dataset))\n",
        "  # ===== Remove the duplicate  =====\n",
        "\n",
        "  # ===== Exporting the dataframe  =====\n",
        "  dataset.to_csv(file_name, index=False)\n",
        "  # ===== Exporting the dataframe  =====\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "ugggms0Irs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buildDataset(datasets,\"dataset.csv\")\n",
        "\n",
        "! cp -r \"/content/dataset.csv\" \"drive/Othercomputers/Il mio Laptop/Universita/[IA] Artificial Intelligence/[HLT] Human Language Technologies/project/Sources/dataset.csv\""
      ],
      "metadata": {
        "id": "rHEHDSNgaD2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4791a56-52b3-443e-f68c-39e05c5392d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  0\n",
            "\tRow deleted from raw-dataset:  10586\n",
            "\tNum of word:  1312283\n",
            "\tNumber of phrases : 48456\n",
            "\n",
            "Dataset:  1\n",
            "\tRow deleted from raw-dataset:  10586\n",
            "\tNum of word:  1312283\n",
            "\tNumber of phrases : 20525\n",
            "\n",
            "Dataset:  2\n",
            "\tRow deleted from raw-dataset:  26771\n",
            "\tNum of word:  2307161\n",
            "\tNumber of phrases : 56235\n",
            "\n",
            "Dataset:  3\n",
            "\tRow deleted from raw-dataset:  26771\n",
            "\tNum of word:  2307161\n",
            "\tNumber of phrases : 43133\n",
            "\n",
            "Number of phrased achieved:  175658\n",
            "Number of duplicated deleted:  105728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset:  0\n",
        "# \tRow deleted from raw-dataset:  10586\n",
        "# \tNum of word:  1312283\n",
        "# \tNumber of phrases : 77974\n",
        "\n",
        "# Dataset:  1\n",
        "# \tRow deleted from raw-dataset:  10586\n",
        "# \tNum of word:  1312283\n",
        "# \tNumber of phrases : 77974\n",
        "\n",
        "# Dataset:  2\n",
        "# \tRow deleted from raw-dataset:  26771\n",
        "# \tNum of word:  2307161\n",
        "# \tNumber of phrases : 236166\n",
        "\n",
        "# Dataset:  3\n",
        "# \tRow deleted from raw-dataset:  26771\n",
        "# \tNum of word:  2307161\n",
        "# \tNumber of phrases : 236166\n",
        "\n",
        "# Number of phrased achieved:  628280\n",
        "# Number of duplicated deleted:  458891"
      ],
      "metadata": {
        "id": "ZdANTit6K7aR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}