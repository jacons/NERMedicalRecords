{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ViT-G1mIgSRr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ed0fzLyOZKRH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "from pandas import DataFrame\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD,Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, time, os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "folder = \"/content/drive/MyDrive/NERforMedicalRecords/\"\n",
        "bert  = \"dbmdz/bert-base-italian-xxl-cased\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(bert)"
      ],
      "metadata": {
        "id": "qgtkYEnrgHUY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of file to take into account\n",
        "datasets = [folder+\"Corpus/anamnesi.a.iob\", folder+\"Corpus/anamnesi.b.iob\", folder+\"Corpus/esami.a.iob\", folder+\"Corpus/esami.b.iob\"]\n",
        "\n",
        "# label to entity tag to recognize\n",
        "labels_to_ids = {'B-ACTI': 0, 'B-BODY': 1, 'B-DISO': 2, 'B-DRUG': 3, 'B-SIGN': 4, 'B-TREA': 5, 'I-ACTI': 6,\n",
        "                 'I-BODY': 7, 'I-DISO': 8, 'I-DRUG': 9, 'I-SIGN': 10, 'I-TREA': 11, 'O': 12}"
      ],
      "metadata": {
        "id": "ADDdl3OklbHu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConLL_parser:\n",
        "  def __init__(self, paths: list):\n",
        "  \n",
        "    self.dataset = [] # List of phrases and labels\n",
        "\n",
        "    # For each file in a list, we parse a file formatted in ConLL\n",
        "    for file_ in paths: self.read_conll(file_)\n",
        "\n",
        "  def read_conll(self, path:str) -> None:\n",
        "\n",
        "    nphrases = 0\n",
        "    print(\"File: \",os.path.basename(path))\n",
        "\n",
        "    with open(path) as f: \n",
        "      phrase,labels = [], [] # lists that contain the words and labels\n",
        "\n",
        "      for line in f.readlines(): # reads word by word\n",
        "        line = line.split() # Trasform a line into array\n",
        "\n",
        "        if len(line) == 0:\n",
        "          # if the \"phrase\" contains at least one word we add to dataset\n",
        "          if (len(phrase)!= 0) & (len(labels)!= 0):\n",
        "            self.dataset.append((\" \".join(phrase),\" \".join(labels)))\n",
        "            nphrases += 1\n",
        "          phrase,labels = [], []\n",
        "\n",
        "        elif line[0] != \"-DOCSTART-\":\n",
        "          phrase.append(line[0]) # Not lemmatized word\n",
        "          labels.append(line[3]) # label that corresponding to the word\n",
        "      print(\"\\tNumber of phrases made: \",nphrases)\n",
        "\n",
        "  def holdout(self, size: float, tr: float = 0.8, vl: float = 0.10, ts:float = 0.10) -> DataFrame:  \n",
        "\n",
        "    \"\"\"\n",
        "    In this phase we transfom a list of pairs (phrase,label) into \"holdout\" dataframe used to model selection\n",
        "      1) Create a unique dataframe.\n",
        "      2) Remove the duplicate (useless for our scope, improve the data dimention)\n",
        "      3) Sampling the dataset in order to work with a subset of all data avaiable\n",
        "      4) Dividing the final dataset base on holdout technique.\n",
        "    \"\"\"\n",
        "\n",
        "    dt = pd.DataFrame(self.dataset,columns=[\"tokens\",\"labels\"]).drop_duplicates().sample(frac=size, random_state=42)\n",
        "    length = len(dt)\n",
        "\n",
        "    return np.split(dt,[int(tr * length), int((tr + vl) * length)])"
      ],
      "metadata": {
        "id": "5LCyIt1yXqDc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  # We try preprocess the data as much as possible.\n",
        "  def __init__(self, dataset: DataFrame):\n",
        "\n",
        "    self.input_ids, self.mask, self.labels = [], [], []\n",
        "\n",
        "    for _,row in dataset.iterrows():\n",
        "      # Apply the tokenization at each row\n",
        "      token_text = tokenizer(row[0], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "      label_ids = self.align_label(token_text.word_ids(),row[1])\n",
        "\n",
        "      # moving the result on GPU\n",
        "      self.input_ids.append(token_text['input_ids'].squeeze(0).to(\"cuda:0\"))\n",
        "      self.mask.append(token_text['attention_mask'].squeeze(0).to(\"cuda:0\"))\n",
        "\n",
        "      self.labels.append(torch.LongTensor(label_ids).to(\"cuda:0\"))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.mask[idx], self.labels[idx]\n",
        "\n",
        "  def align_label(self, token: list, labels: str):\n",
        "    \n",
        "    labels = labels.split() # Trasforming a string of label into array\n",
        "\n",
        "    # We take into cosideration the previous word to identify if the id is already seen\n",
        "    previous_id = None\n",
        "    label_ids = [] # Aligned labels\n",
        "\n",
        "    # We can all ids in the token and we try to associate to a label  \n",
        "    for word_idx in token:\n",
        "  \n",
        "      if word_idx is None: # typically when we encounter [CLS]\n",
        "        label_ids.append(-100)\n",
        "      else:\n",
        "        try: # We try to associate a label\n",
        "          label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "        except:\n",
        "          label_ids.append(-100)\n",
        "  \n",
        "      previous_id = word_idx \n",
        "\n",
        "    return label_ids"
      ],
      "metadata": {
        "id": "Og5GMB1zi-Mf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,frozen=True):\n",
        "        super(BertModel, self).__init__()\n",
        "        self.bert = AutoModelForMaskedLM.from_pretrained(bert,num_labels=13)\n",
        "        if frozen:\n",
        "          for param in self.bert.bert.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_id, mask, label):\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2XSDP0I6joeu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, df_train: DataFrame, df_val: DataFrame, batch_size: int, lr: float, epochs: int):\n",
        "  \n",
        "  # We create a iterator for training e validation dataset\n",
        "  tr = DataLoader(MyDataset(df_train), batch_size=batch_size)\n",
        "  vl = DataLoader(MyDataset(df_val), batch_size=batch_size)\n",
        "\n",
        "  tr_size, vl_size = len(tr), len(vl)\n",
        "\n",
        "  optimizer = SGD(model.parameters(), lr=lr)\n",
        "\n",
        "  for e in range(epochs):  \n",
        "\n",
        "    acc_train, loss_train = 0, 0\n",
        "    acc_val, loss_val = 0, 0\n",
        "\n",
        "    model.train() # Traininig phase\n",
        "    for input_id, mask, tr_label in tqdm(tr):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss, logits = model(input_id, mask, tr_label)\n",
        "\n",
        "      for i in range(logits.shape[0]):\n",
        "        logits_clean = logits[i][tr_label[i] != -100]\n",
        "        label_clean = tr_label[i][tr_label[i] != -100]\n",
        "\n",
        "        predictions = logits_clean.argmax(dim=1)\n",
        "        acc = (predictions == label_clean).float().mean()\n",
        "        acc_train += acc\n",
        "        loss_train += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    torch.save(model.state_dict(), \"./model.pt\")\n",
        "\n",
        "    model.eval() # Validation phase\n",
        "    for input_id, mask, val_label in tqdm(vl):\n",
        "      \n",
        "      loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "      for i in range(logits.shape[0]):\n",
        "        logits_clean = logits[i][val_label[i] != -100]\n",
        "        label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "        predictions = logits_clean.argmax(dim=1)\n",
        "        acc = (predictions == label_clean).float().mean()\n",
        "        acc_val += acc\n",
        "        loss_val += loss.item()\n",
        "      \n",
        "    tr_accuracy, tr_loss = (acc_train / tr_size), (loss_train / tr_size)\n",
        "    val_accuracy, val_loss = (acc_val / vl_size), (loss_val / vl_size)\n",
        "     \n",
        "    print(f'Epochs: {e + 1} | Loss: {tr_loss: .3f} | Accuracy: {tr_accuracy: .3f} | Val_Loss: {val_loss: .3f} | Accuracy: {val_accuracy: .3f}')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "4tsxc14Uq52c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = ConLL_parser(datasets)\n",
        "df_train,df_val, _ = parser.holdout(size=0.5)\n",
        "\n",
        "model = BertModel().to(\"cuda:0\")\n",
        "train(model,df_train, df_val,2,5e-3,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXZ-COyxtgjj",
        "outputId": "b2a4105a-bce5-418b-938b-22c2dd7a7b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33510/33510 [1:00:48<00:00,  9.19it/s]\n",
            "100%|██████████| 4189/4189 [06:35<00:00, 10.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Loss:  0.325 | Accuracy:  0.916 | Val_Loss:  0.269 | Accuracy:  0.916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 29206/33510 [52:51<07:48,  9.18it/s]"
          ]
        }
      ]
    }
  ]
}